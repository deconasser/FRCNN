{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deconasser/SimpleCV/blob/main/Unetv5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummary\n",
        "!pip install torchgeometry"
      ],
      "metadata": {
        "id": "uPtD4ZoQZ58X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "bdf045e1-2bd2-4bcd-e8c2-dd13e27445cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
            "Collecting torchgeometry\n",
            "  Downloading torchgeometry-0.1.2-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from torchgeometry) (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.0.0->torchgeometry)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.0.0->torchgeometry)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.0.0->torchgeometry)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.0.0->torchgeometry)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.0.0->torchgeometry)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.0.0->torchgeometry)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.0.0->torchgeometry)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.0.0->torchgeometry)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.0.0->torchgeometry)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.0.0->torchgeometry)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.0.0->torchgeometry)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchgeometry) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->torchgeometry)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->torchgeometry) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->torchgeometry) (1.3.0)\n",
            "Downloading torchgeometry-0.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchgeometry\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 torchgeometry-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbJ8M3TIUBPU",
        "outputId": "a1f6a4e9-6a0b-45ff-a80d-4f2154d0e19a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation-models-pytorch"
      ],
      "metadata": {
        "id": "1yud4_-9ZjWM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "87d8fe69-ec33-4474-c48a-37bd1d1bc640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting segmentation-models-pytorch\n",
            "  Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.18.1+cu121)\n",
            "Collecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch)\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/58.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch)\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting timm==0.9.2 (from segmentation-models-pytorch)\n",
            "  Downloading timm-0.9.2-py3-none-any.whl.metadata (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (4.66.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (9.4.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.3.1+cu121)\n",
            "Collecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation-models-pytorch) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation-models-pytorch) (0.23.5)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation-models-pytorch) (0.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.5.82)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\n",
            "Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16425 sha256=69ec661e72d2f49535ec658c54dae456e302e611c042084e233a322f170868ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60944 sha256=96c72a3da8270bb0ff9ae2e2fb59b2849c1c213559d2948d939e695bbf51f658\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.3 timm-0.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "from torchgeometry.losses import one_hot\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import time\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision.transforms import Resize, PILToTensor, ToPILImage, Compose, InterpolationMode\n",
        "from collections import OrderedDict\n",
        "torch.set_printoptions(profile=\"default\")"
      ],
      "metadata": {
        "id": "r9KCWCi_ZoVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G7ntedRpgvhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "model = smp.PAN(\n",
        "    encoder_name=\"timm-mobilenetv3_large_100\",\n",
        "    encoder_weights=\"imagenet\",\n",
        "    in_channels=3,\n",
        "    classes=3\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnJRud4zSTLw",
        "outputId": "4cd26493-ab9e-435e-f7ef-25977357dd95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mobilenetv3_large_100-427764d5.pth\" to /root/.cache/torch/hub/checkpoints/tf_mobilenetv3_large_100-427764d5.pth\n",
            "100%|██████████| 21.1M/21.1M [00:00<00:00, 142MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of class in the data set (3: neoplastic, non neoplastic, background)\n",
        "num_classes = 3\n",
        "\n",
        "# Number of epochs\n",
        "epochs = 50\n",
        "\n",
        "# Hyperparameters for training\n",
        "learning_rate = 1e-04\n",
        "batch_size = 4\n",
        "display_step = 50\n",
        "\n",
        "# Model path\n",
        "checkpoint_path = '/content/drive/MyDrive/Unet/'\n",
        "pretrained_path = \"/content/drive/MyDrive/Unet/\"\n",
        "\n",
        "# Initialize lists to keep track of loss and accuracy\n",
        "loss_epoch_array = []\n",
        "train_accuracy = []\n",
        "test_accuracy = []\n",
        "valid_accuracy = []"
      ],
      "metadata": {
        "id": "Ohv15l9HSmRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = Compose([Resize((512, 512), interpolation=InterpolationMode.BILINEAR),\n",
        "                     PILToTensor()])"
      ],
      "metadata": {
        "id": "bk8dBlt5S7su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UNetDataClass(Dataset):\n",
        "    def __init__(self, images_path, masks_path, transform):\n",
        "        super(UNetDataClass, self).__init__()\n",
        "\n",
        "        images_list = os.listdir(images_path)\n",
        "        masks_list = os.listdir(masks_path)\n",
        "\n",
        "        images_list = [images_path + image_name for image_name in images_list]\n",
        "        masks_list = [masks_path + mask_name for mask_name in masks_list]\n",
        "\n",
        "        self.images_list = images_list\n",
        "        self.masks_list = masks_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.images_list[index]\n",
        "        mask_path = self.masks_list[index]\n",
        "\n",
        "        # Open image and mask\n",
        "        data = Image.open(img_path)\n",
        "        label = Image.open(mask_path)\n",
        "\n",
        "\n",
        "\n",
        "        # Normalize\n",
        "        data = self.transform(data) / 255\n",
        "        label = self.transform(label) / 255\n",
        "\n",
        "\n",
        "        label = torch.where(label>0.65, 1.0, 0.0)\n",
        "\n",
        "        label[2, :, :] = 0.0001\n",
        "        label = torch.argmax(label, 0).type(torch.int64)\n",
        "\n",
        "        return data, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_list)"
      ],
      "metadata": {
        "id": "9e6wvVfQS8iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_path = \"/content/drive/MyDrive/Unet/data/train/\"\n",
        "masks_path =  \"/content/drive/MyDrive/Unet/data/train_gt/\""
      ],
      "metadata": {
        "id": "ygFlxdl7TISU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unet_dataset = UNetDataClass(images_path, masks_path, transform)\n",
        "unet_dataset.__getitem__(20)"
      ],
      "metadata": {
        "id": "zUT9JWGbTJUm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a13b6462-70ac-48ce-b5ff-029221aa8199"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.0078, 0.0039, 0.0039,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0078, 0.0039, 0.0039,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0039, 0.0039, 0.0078,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          ...,\n",
              "          [0.0667, 0.0667, 0.0667,  ..., 0.0627, 0.0549, 0.0510],\n",
              "          [0.0784, 0.0784, 0.0784,  ..., 0.0667, 0.0627, 0.0627],\n",
              "          [0.0706, 0.0706, 0.0706,  ..., 0.0549, 0.0549, 0.0549]],\n",
              " \n",
              "         [[0.0078, 0.0039, 0.0039,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0078, 0.0039, 0.0039,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0039, 0.0039, 0.0078,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          ...,\n",
              "          [0.1216, 0.1216, 0.1216,  ..., 0.1255, 0.1294, 0.1333],\n",
              "          [0.1333, 0.1333, 0.1333,  ..., 0.1255, 0.1294, 0.1294],\n",
              "          [0.1255, 0.1255, 0.1255,  ..., 0.1098, 0.1098, 0.1098]],\n",
              " \n",
              "         [[0.0078, 0.0039, 0.0039,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0078, 0.0039, 0.0039,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          [0.0039, 0.0039, 0.0078,  ..., 0.0000, 0.0000, 0.0000],\n",
              "          ...,\n",
              "          [0.1216, 0.1216, 0.1216,  ..., 0.1137, 0.1137, 0.1137],\n",
              "          [0.1333, 0.1333, 0.1333,  ..., 0.1216, 0.1255, 0.1216],\n",
              "          [0.1255, 0.1255, 0.1255,  ..., 0.1137, 0.1137, 0.1137]]]),\n",
              " tensor([[2, 2, 2,  ..., 2, 2, 2],\n",
              "         [2, 2, 2,  ..., 2, 2, 2],\n",
              "         [2, 2, 2,  ..., 2, 2, 2],\n",
              "         ...,\n",
              "         [2, 2, 2,  ..., 2, 2, 2],\n",
              "         [2, 2, 2,  ..., 2, 2, 2],\n",
              "         [2, 2, 2,  ..., 2, 2, 2]]))"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 0.9\n",
        "valid_size = 0.1\n",
        "torch.manual_seed(42)\n",
        "train_set, valid_set = random_split(unet_dataset,\n",
        "                                    [int(train_size * len(unet_dataset)) ,\n",
        "                                     int(valid_size * len(unet_dataset))])"
      ],
      "metadata": {
        "id": "EvYvR14Qgx5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid_set, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "brkFGKBOg9Bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from albumentations import (\n",
        "    Compose,\n",
        "    RandomRotate90,\n",
        "    Flip,\n",
        "    Transpose,\n",
        "    ElasticTransform,\n",
        "    GridDistortion,\n",
        "    OpticalDistortion,\n",
        "    RandomBrightnessContrast,\n",
        "    HorizontalFlip,\n",
        "    VerticalFlip,\n",
        "    RandomGamma,\n",
        "    RGBShift,\n",
        ")"
      ],
      "metadata": {
        "id": "vDKhwm2ChYjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmentation = Compose([\n",
        "    HorizontalFlip(p=0.5),\n",
        "    VerticalFlip(p=0.5),\n",
        "    RandomGamma (gamma_limit=(70, 130), eps=None, always_apply=False, p=0.2),\n",
        "    RGBShift(p=0.3, r_shift_limit=10, g_shift_limit=10, b_shift_limit=10),\n",
        "])"
      ],
      "metadata": {
        "id": "7n_MloSHhhIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SegDataClass(Dataset):\n",
        "    def __init__(self, images_path, masks_path, transform=None, augmentation=None):\n",
        "        super(SegDataClass, self).__init__()\n",
        "\n",
        "        images_list = os.listdir(images_path)\n",
        "        masks_list = os.listdir(masks_path)\n",
        "\n",
        "        images_list = [os.path.join(images_path, image_name) for image_name in images_list]\n",
        "        masks_list = [os.path.join(masks_path, mask_name) for mask_name in masks_list]\n",
        "\n",
        "        self.images_list = images_list\n",
        "        self.masks_list = masks_list\n",
        "        self.transform = transform\n",
        "        self.augmentation = augmentation\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.images_list[index]\n",
        "        mask_path = self.masks_list[index]\n",
        "\n",
        "        # Open image and mask\n",
        "        data = Image.open(img_path)\n",
        "        label = Image.open(mask_path)\n",
        "\n",
        "        # Augmentation\n",
        "        if self.augmentation:\n",
        "            augmented = self.augmentation(image=np.array(data), mask=np.array(label))\n",
        "            data = Image.fromarray(augmented['image'])\n",
        "            label = Image.fromarray(augmented['mask'])\n",
        "\n",
        "        # Normalize\n",
        "        data = self.transform(data) / 255\n",
        "        label = self.transform(label) / 255\n",
        "\n",
        "        label = torch.where(label > 0.65, 1.0, 0.0)\n",
        "        label[2, :, :] = 0.0001\n",
        "        label = torch.argmax(label, 0).type(torch.int64)\n",
        "\n",
        "        return data, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_list)\n",
        "\n",
        "\n",
        "# transform = transforms.ToTensor()\n",
        "aug_dataset = SegDataClass(images_path, masks_path, transform=transform, augmentation=augmentation)"
      ],
      "metadata": {
        "id": "YWkhD7DxhjwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "train_aug_set, valid_aug_set = random_split(aug_dataset,\n",
        "                                    [int(train_size * len(aug_dataset)) ,\n",
        "                                     int(valid_size * len(aug_dataset))])\n",
        "\n",
        "from torch.utils.data import ConcatDataset\n",
        "# combined_dataset = ConcatDataset([aug_dataset, train_set])\n",
        "# train_dataloader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True)\n",
        "train_dataloader = DataLoader(train_aug_set, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "BeVYjKVkiI7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CEDiceLoss(nn.Module):\n",
        "    def __init__(self, weights) -> None:\n",
        "        super(CEDiceLoss, self).__init__()\n",
        "        self.eps: float = 1e-6\n",
        "        self.weights: torch.Tensor = weights\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            input: torch.Tensor,\n",
        "            target: torch.Tensor) -> torch.Tensor:\n",
        "        if not torch.is_tensor(input):\n",
        "            raise TypeError(\"Input type is not a torch.Tensor. Got {}\"\n",
        "                            .format(type(input)))\n",
        "        if not len(input.shape) == 4:\n",
        "            raise ValueError(\"Invalid input shape, we expect BxNxHxW. Got: {}\"\n",
        "                             .format(input.shape))\n",
        "        if not input.shape[-2:] == target.shape[-2:]:\n",
        "            raise ValueError(\"input and target shapes must be the same. Got: {}\"\n",
        "                             .format(input.shape, input.shape))\n",
        "        if not input.device == target.device:\n",
        "            raise ValueError(\n",
        "                \"input and target must be in the same device. Got: {}\" .format(\n",
        "                    input.device, target.device))\n",
        "        if not self.weights.shape[1] == input.shape[1]:\n",
        "            raise ValueError(\"The number of weights must equal the number of classes\")\n",
        "        if not torch.sum(self.weights).item() == 1:\n",
        "            raise ValueError(\"The sum of all weights must equal 1\")\n",
        "\n",
        "        # cross entropy loss\n",
        "        celoss = nn.CrossEntropyLoss(self.weights)(input, target)\n",
        "\n",
        "        # compute softmax over the classes axis\n",
        "        input_soft = F.softmax(input, dim=1)\n",
        "\n",
        "        # create the labels one hot tensor\n",
        "        target_one_hot = one_hot(target, num_classes=input.shape[1],\n",
        "                                 device=input.device, dtype=input.dtype)\n",
        "\n",
        "        # compute the actual dice score\n",
        "        dims = (2, 3)\n",
        "        intersection = torch.sum(input_soft * target_one_hot, dims)\n",
        "        cardinality = torch.sum(input_soft + target_one_hot, dims)\n",
        "\n",
        "        dice_score = 2. * intersection / (cardinality + self.eps)\n",
        "\n",
        "        dice_score = torch.sum(dice_score * self.weights, dim=1)\n",
        "\n",
        "        return torch.mean(1. - dice_score) + celoss\n",
        "#         return dice_score"
      ],
      "metadata": {
        "id": "C0sKQgLkiSPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        residual = self.conv1(residual)\n",
        "        residual = self.bn3(residual)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "uW9uW3MRjHrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class encoder_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(encoder_block, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding='same')\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding='same')\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        next_layer = self.max_pool(x)\n",
        "        skip_layer = x\n",
        "\n",
        "        return next_layer, skip_layer"
      ],
      "metadata": {
        "id": "5jbCTmTJ0Ryc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class res_encoder_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(res_encoder_block, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding='same')\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding='same')\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        residual = self.conv1(residual)\n",
        "        residual = self.bn3(residual)\n",
        "        x += residual\n",
        "        x = self.relu(x)\n",
        "\n",
        "        next_layer = self.max_pool(x)\n",
        "        skip_layer = x\n",
        "\n",
        "        return next_layer, skip_layer"
      ],
      "metadata": {
        "id": "u5o5Ln3o0Tgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class decoder_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(decoder_block, self).__init__()\n",
        "\n",
        "        self.transpose_conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(2 * out_channels, out_channels, kernel_size=3, stride=1, padding='same')\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding='same')\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "\n",
        "    def forward(self, x, skip_layer):\n",
        "        x = self.transpose_conv(x)\n",
        "        x = torch.cat([x, skip_layer], axis=1)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "Rw2JZb4q0X4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class res_decoder_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(res_decoder_block, self).__init__()\n",
        "\n",
        "        self.transpose_conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(2 * out_channels, out_channels, kernel_size=3, stride=1, padding='same')\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding='same')\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "\n",
        "    def forward(self, x, skip_layer):\n",
        "        x = self.transpose_conv(x)\n",
        "        x = torch.cat([x, skip_layer], axis=1)\n",
        "\n",
        "        residual = x\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        residual = self.conv1(residual)\n",
        "        residual = self.bn3(residual)\n",
        "\n",
        "        x += residual\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "uirM5vKY0YoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class bottleneck_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(bottleneck_block, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding='same')\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding='same')\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "LHFEmq-s0dDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UNet model\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_class=3):\n",
        "        super(UNet, self).__init__()\n",
        "        # Encoder blocks\n",
        "        self.enc1 = encoder_block(3, 64)\n",
        "        self.enc2 = encoder_block(64, 128)\n",
        "        self.enc3 = res_encoder_block(128, 256)\n",
        "        self.enc4 = encoder_block(256, 512)\n",
        "\n",
        "        # Bottleneck block\n",
        "        self.bottleneck = ResidualBlock(512, 1024)\n",
        "\n",
        "        # Decoder blocks\n",
        "        self.dec1 = decoder_block(1024, 512)\n",
        "        self.dec2 = res_decoder_block(512, 256)\n",
        "        self.dec3 = decoder_block(256, 128)\n",
        "        self.dec4 = decoder_block(128, 64)\n",
        "\n",
        "        # 1x1 convolution\n",
        "        self.out = nn.Conv2d(64, n_class, kernel_size=1, padding='same')\n",
        "\n",
        "    def forward(self, image):\n",
        "        n1, s1 = self.enc1(image)\n",
        "        n2, s2 = self.enc2(n1)\n",
        "        n3, s3 = self.enc3(n2)\n",
        "        n4, s4 = self.enc4(n3)\n",
        "\n",
        "        n5 = self.bottleneck(n4)\n",
        "\n",
        "        n6 = self.dec1(n5, s4)\n",
        "        n7 = self.dec2(n6, s3)\n",
        "        n8 = self.dec3(n7, s2)\n",
        "        n9 = self.dec4(n8, s1)\n",
        "\n",
        "        output = self.out(n9)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "sz2r4Xsv0fM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n"
      ],
      "metadata": {
        "id": "pHnhJicX0kso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weights_init(model):\n",
        "    if isinstance(model, nn.Linear):\n",
        "        # Xavier Distribution\n",
        "        torch.nn.init.xavier_uniform_(model.weight)"
      ],
      "metadata": {
        "id": "5udDVe5c0rYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, optimizer, path):\n",
        "    checkpoint = {\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, path)\n",
        "\n",
        "def load_model(model, optimizer, path):\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint[\"model\"])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    return model, optimizer"
      ],
      "metadata": {
        "id": "DNBGNN-70t8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train function for each epoch\n",
        "def train(train_dataloader, valid_dataloader,learing_rate_scheduler, epoch, display_step):\n",
        "    print(f\"Start epoch #{epoch+1}, learning rate for this epoch: {learing_rate_scheduler.get_last_lr()}\")\n",
        "    start_time = time.time()\n",
        "    train_loss_epoch = 0\n",
        "    test_loss_epoch = 0\n",
        "    last_loss = 999999999\n",
        "    model.train()\n",
        "    for i, (data,targets) in enumerate(train_dataloader):\n",
        "\n",
        "        # Load data into GPU\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "\n",
        "        # Backpropagation, compute gradients\n",
        "        loss = loss_function(outputs, targets.long())\n",
        "        loss.backward()\n",
        "\n",
        "        # Apply gradients\n",
        "        optimizer.step()\n",
        "\n",
        "        # Save loss\n",
        "        train_loss_epoch += loss.item()\n",
        "        if (i+1) % display_step == 0:\n",
        "#             accuracy = float(test(test_loader))\n",
        "            print('Train Epoch: {} [{}/{} ({}%)]\\tLoss: {:.4f}'.format(\n",
        "                epoch + 1, (i+1) * len(data), len(train_dataloader.dataset), 100 * (i+1) * len(data) / len(train_dataloader.dataset),\n",
        "                loss.item()))\n",
        "\n",
        "    print(f\"Done epoch #{epoch+1}, time for this epoch: {time.time()-start_time}s\")\n",
        "    train_loss_epoch/= (i + 1)\n",
        "\n",
        "    # Evaluate the validation set\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data, target in valid_dataloader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            test_output = model(data)\n",
        "            test_loss = loss_function(test_output, target)\n",
        "            test_loss_epoch += test_loss.item()\n",
        "\n",
        "    test_loss_epoch/= (i+1)\n",
        "\n",
        "    return train_loss_epoch , test_loss_epoch"
      ],
      "metadata": {
        "id": "2hB11TNG0yEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test function\n",
        "def test(dataloader):\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (data, targets) in enumerate(dataloader):\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            outputs = model(data)\n",
        "            _, pred = torch.max(outputs, 1)\n",
        "            test_loss += targets.size(0)\n",
        "            correct += torch.sum(pred == targets).item()\n",
        "    return 100.0 * correct / test_loss"
      ],
      "metadata": {
        "id": "atUKuqF505Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = Unet(in_channels=3, num_classes = 3)\n",
        "\n",
        "try:\n",
        "    checkpoint = torch.load(pretrained_path)\n",
        "\n",
        "    new_state_dict = OrderedDict()\n",
        "    for k, v in checkpoint['model'].items():\n",
        "        name = k[7:] # remove `module.`\n",
        "        new_state_dict[name] = v\n",
        "    # load params\n",
        "    model.load_state_dict(new_state_dict)\n",
        "    model = nn.DataParallel(model)\n",
        "    model.to(device)\n",
        "except:\n",
        "    model.apply(weights_init)\n",
        "    model = nn.DataParallel(model)\n",
        "    model.to(device)"
      ],
      "metadata": {
        "id": "Be1WyQxZ06F0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torch.Tensor([[0.4, 0.55, 0.05]]).cuda()\n",
        "loss_function = CEDiceLoss(weights)\n",
        "\n",
        "# Define the optimizer (Adam optimizer)\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=learning_rate)\n",
        "try:\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Learning rate scheduler\n",
        "learing_rate_scheduler = lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.6)"
      ],
      "metadata": {
        "id": "pRsZPCxc09Ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_model(model, optimizer, checkpoint_path)"
      ],
      "metadata": {
        "id": "VRv2R9_f0_Fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_array = []\n",
        "test_loss_array = []\n",
        "last_loss = 9999999999999\n",
        "for epoch in range(epochs):\n",
        "    train_loss_epoch = 0\n",
        "    test_loss_epoch = 0\n",
        "    (train_loss_epoch, test_loss_epoch) = train(train_dataloader,\n",
        "                                              valid_dataloader,\n",
        "                                              learing_rate_scheduler, epoch, display_step)\n",
        "\n",
        "    if test_loss_epoch < last_loss:\n",
        "        save_model(model, optimizer, checkpoint_path)\n",
        "        last_loss = test_loss_epoch\n",
        "\n",
        "    learing_rate_scheduler.step()\n",
        "    train_loss_array.append(train_loss_epoch)\n",
        "    test_loss_array.append(test_loss_epoch)"
      ],
      "metadata": {
        "id": "-8cRCPSo1GEc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}